import streamlit as st
import os
import json
import requests  # Make sure requests is imported
from datetime import datetime, timedelta
import google.generativeai as genai
from huggingface_hub import InferenceClient
import time
import random

# Page config
st.set_page_config(
    page_title="LinkedIn Content Intelligence Agent",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-size: 2.5rem;
    font-weight: bold;
    text-align: center;
    margin-bottom: 2rem;
}

.admin-header {
    background: linear-gradient(90deg, #f093fb 0%, #f5576c 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-size: 2rem;
    font-weight: bold;
    text-align: center;
    margin-bottom: 1.5rem;
}

.research-container {
    word-wrap: break-word;
    overflow-wrap: break-word;
    max-width: 100%;
    padding: 1rem;
    background: rgba(255, 255, 255, 0.05);
    border-radius: 8px;
    margin: 10px 0;
}

.live-indicator {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 4px 8px;
    background: rgba(34, 197, 94, 0.1);
    border-radius: 12px;
    font-size: 0.8rem;
    color: #22c55e;
    margin-bottom: 10px;
}

.live-dot {
    width: 8px;
    height: 8px;
    background: #22c55e;
    border-radius: 50%;
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0% { opacity: 1; transform: scale(1); }
    50% { opacity: 0.6; transform: scale(1.2); }
    100% { opacity: 1; transform: scale(1); }
}

.demo-indicator {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 4px 8px;
    background: rgba(59, 130, 246, 0.1);
    border-radius: 12px;
    font-size: 0.8rem;
    color: #3b82f6;
    margin-bottom: 10px;
}

.demo-dot {
    width: 8px;
    height: 8px;
    background: #3b82f6;
    border-radius: 50%;
    animation: blink 2s infinite;
}

@keyframes blink {
    0% { opacity: 1; }
    50% { opacity: 0.3; }
    100% { opacity: 1; }
}

.status-good {
    background-color: #d4edda;
    border: 1px solid #c3e6cb;
    color: #155724;
    padding: 10px;
    border-radius: 5px;
    margin: 5px 0;
}

.status-warning {
    background-color: #fff3cd;
    border: 1px solid #ffeaa7;
    color: #856404;
    padding: 10px;
    border-radius: 5px;
    margin: 5px 0;
}

.status-error {
    background-color: #f8d7da;
    border: 1px solid #f5c6cb;
    color: #721c24;
    padding: 10px;
    border-radius: 5px;
    margin: 5px 0;
}

.metric-card {
    background: white;
    padding: 1rem;
    border-radius: 10px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    margin: 0.5rem 0;
}
</style>
""", unsafe_allow_html=True)

class FreeAPIManager:
    """Manages free-tier API calls with intelligent rate limiting"""
    
    def __init__(self):
        self.hf_client = None
        self.gemini_model = None
        self.usage_file = 'api_usage.json'
        
        # Initialize APIs
        self._setup_apis()
    
    def _setup_apis(self):
        """Set up free API clients"""
        try:
            # Hugging Face setup
            hf_token = st.secrets.get('HUGGINGFACE_TOKEN')
            if hf_token:
                self.hf_client = InferenceClient(token=hf_token)
            
            # Gemini setup with explicit model check
            gemini_key = st.secrets.get('GEMINI_API_KEY')
            if gemini_key:
                genai.configure(api_key=gemini_key)
                # Use the correct current model name
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        except Exception as e:
            st.error(f"API setup error: {e}")
    
    def check_daily_limit(self, api_name: str) -> bool:
        """Check if we're under daily API limits"""
        try:
            if 'api_usage' not in st.session_state:
                st.session_state.api_usage = {}
            
            usage = st.session_state.api_usage
            today = datetime.now().strftime('%Y-%m-%d')
            daily_usage = usage.get(today, {})
            
            # Conservative daily limits
            limits = {
                'huggingface': 25,  # From 1000/month free tier
                'gemini': 80,       # From 15/minute rate limit
            }
            
            current_usage = daily_usage.get(api_name, 0)
            return current_usage < limits[api_name]
        except:
            return True  # Default to allowing if error
    
    def log_api_usage(self, api_name: str):
        """Log API usage for rate limiting"""
        try:
            if 'api_usage' not in st.session_state:
                st.session_state.api_usage = {}
            
            today = datetime.now().strftime('%Y-%m-%d')
            if today not in st.session_state.api_usage:
                st.session_state.api_usage[today] = {}
            
            st.session_state.api_usage[today][api_name] = st.session_state.api_usage[today].get(api_name, 0) + 1
        except Exception as e:
            st.error(f"Usage logging error: {e}")
    
    def research_with_gemini(self, topic: str) -> dict:
        """Use Gemini for research with simple, engaging output"""
        if not self.gemini_model or not self.check_daily_limit('gemini'):
            return None
        
        try:
            prompt = f"""Research "{topic}" and explain it like you're talking to a friend over coffee. 

Write at a 7th grade reading level using:
- Short, simple sentences (max 15 words each)
- Common, everyday words (avoid jargon)
- Conversational tone - like explaining to a neighbor
- 2-3 short paragraphs maximum
- Make it interesting and easy to understand

Include:
- What's happening right now with {topic}
- Why people should care
- One surprising fact or trend

Write like a human, not a textbook. Keep it simple and engaging.

Topic: {topic}"""

            response = self.gemini_model.generate_content(prompt)
            self.log_api_usage('gemini')
            
            # Clean the response to remove any unwanted bracketed content
            clean_text = self._clean_gemini_response(response.text)
            
            # Return structured data
            result = {
                "research_summary": clean_text,
                "source": "Gemini AI Research",
                "timestamp": datetime.now().isoformat(),
                "topic": topic
            }
            
            return result
            
        except Exception as e:
            return None
    
    def _clean_gemini_response(self, text: str) -> str:
        """Clean Gemini response to remove bracketed placeholders and ensure LinkedIn-ready content"""
        import re
        
        # Remove common bracketed placeholders
        patterns_to_remove = [
            r'\[verify.*?\]',
            r'\[source.*?\]', 
            r'\[citation.*?\]',
            r'\[upload.*?\]',
            r'\[image.*?\]',
            r'\[insert.*?\]',
            r'\[add.*?\]',
            r'\[check.*?\]',
            r'\[confirm.*?\]',
            r'\[.*?needed.*?\]'
        ]
        
        cleaned = text
        for pattern in patterns_to_remove:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # Remove extra whitespace and clean up formatting
        cleaned = re.sub(r'\s+', ' ', cleaned)  # Multiple spaces to single space
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)  # Multiple newlines to double newline
        cleaned = cleaned.strip()
        
        return cleaned
    
    def generate_content_with_hf(self, topic: str, research_data: dict = None) -> str:
        """Generate LinkedIn content - using cached examples due to HF API limitations"""
        # For now, skip HF API and use intelligent content generation
        try:
            research_context = ""
            if research_data and "research_summary" in research_data:
                research_context = research_data["research_summary"][:200]
            
            # Generate professional LinkedIn content based on research
            if research_context:
                # Use the research to create contextual content
                content = self._generate_contextual_content(topic, research_context)
            else:
                # Use topic-based templates
                content = self._generate_template_content(topic)
            
            self.log_api_usage('huggingface')  # Track usage for consistency
            return content
            
        except Exception as e:
            return None
    
    def _generate_contextual_content(self, topic: str, research: str) -> str:
        """Generate content using research insights"""
        # Extract key insights from research
        lines = research.split('\n')
        key_insight = lines[0][:100] if lines else f"Innovation in {topic} is accelerating"
        
        templates = [
            f"üöÄ {key_insight}\n\nThe landscape is shifting rapidly. Companies that adapt early will lead tomorrow's market.\n\nWhat trends are you seeing in {topic}?\n\n#{topic.replace(' ', '')} #Innovation #Growth #Future",
            f"üí° Insight: {key_insight}\n\nThis changes everything we thought we knew about {topic}. The implications for business are huge.\n\nHow is this affecting your industry?\n\n#{topic.replace(' ', '')} #Business #Trends #Strategy",
            f"üìä Data shows: {key_insight}\n\nThe numbers don't lie. {topic} is transforming faster than most people realize.\n\nAre you prepared for what's coming next?\n\n#{topic.replace(' ', '')} #Data #Transformation #Leadership"
        ]
        
        import random
        return random.choice(templates)
    
    def _generate_template_content(self, topic: str) -> str:
        """Generate template content for topics without research"""
        templates = [
            f"Excited to share thoughts on {topic}! üöÄ\n\nThis space is evolving rapidly, and the opportunities are endless. Companies that embrace change will thrive.\n\nWhat's your take on the future of {topic}?\n\n#{topic.replace(' ', '')} #Innovation #Growth #Future",
            f"The {topic} landscape is fascinating right now üí°\n\nWe're seeing unprecedented innovation and disruption. The next 12 months will be critical.\n\nHow are you adapting to these changes?\n\n#{topic.replace(' ', '')} #Business #Strategy #Adaptation",
            f"Been diving deep into {topic} lately üìä\n\nThe data tells a compelling story about where this industry is headed. Exciting times ahead.\n\nWhat trends are you watching in this space?\n\n#{topic.replace(' ', '')} #Trends #Analysis #Growth"
        ]
        
        import random
        return random.choice(templates)
    
    def get_api_status(self) -> dict:
        """Get current API status and usage"""
        status = {
            "gemini": {
                "available": self.gemini_model is not None,
                "under_limit": self.check_daily_limit('gemini'),
                "usage_today": self._get_today_usage('gemini')
            },
            "huggingface": {
                "available": self.hf_client is not None,
                "under_limit": self.check_daily_limit('huggingface'),
                "usage_today": self._get_today_usage('huggingface')
            }
        }
        return status
    
    def _get_today_usage(self, api_name: str) -> int:
        """Get today's usage for an API"""
        try:
            if 'api_usage' not in st.session_state:
                return 0
            today = datetime.now().strftime('%Y-%m-%d')
            return st.session_state.api_usage.get(today, {}).get(api_name, 0)
        except:
            return 0

def load_cached_examples():
    """Load cached examples for demo purposes"""
    examples = {
        "AI in Business": {
            "research": {
                "trends": ["AI automation increasing 40% year-over-year", "SMBs adopting AI tools at record pace"],
                "statistics": "73% of executives plan to increase AI investment in 2024",
                "insights": "Companies using AI see 15% productivity gains on average",
                "impact": "AI democratization enabling small businesses to compete with enterprises"
            },
            "content": """ü§ñ The AI revolution isn't coming‚Äîit's here, and it's changing how small businesses compete.

New data shows 73% of executives are doubling down on AI investment this year. Why? Companies using AI tools see an average 15% productivity boost.

But here's what excites me: AI democratization. Tools that once required PhD teams are now accessible to solo entrepreneurs. The playing field is leveling.

The question isn't whether your business will use AI‚Äîit's whether you'll lead or follow.

What AI tool has surprised you most this year? 

#ArtificialIntelligence #SmallBusiness #Innovation #Productivity #TechTrends"""
        },
        "Remote Work Trends": {
            "research": {
                "trends": ["Hybrid work models becoming permanent", "Focus on productivity over presence"],
                "statistics": "68% of companies adopting permanent flexible work policies",
                "insights": "Remote-first companies report 22% higher employee satisfaction"
            },
            "content": """üìç Remote work isn't a pandemic trend‚Äîit's the future of work, and the data proves it.

68% of companies just made flexible work permanent. But here's the real story: remote-first companies report 22% higher employee satisfaction.

We've moved from "where you work" to "how well you work." Results matter more than desk time.

The companies thriving? Those who invested in digital collaboration tools and trust-based management.

How has remote work changed your productivity? Share your biggest lesson learned.

#RemoteWork #FutureOfWork #Productivity #WorkLifeBalance #Leadership"""
        }
    }
    return examples

def check_admin_access():
    """Check if user should see admin interface"""
    query_params = st.query_params
    return query_params.get("admin", "false") == "true"

def admin_login():
    """Handle admin authentication"""
    if 'admin_authenticated' not in st.session_state:
        st.session_state.admin_authenticated = False
    
    if not st.session_state.admin_authenticated:
        st.markdown('<h2 class="admin-header">üîß Admin Login</h2>', unsafe_allow_html=True)
        
        with st.form("admin_login"):
            password = st.text_input("Admin Password", type="password")
            submitted = st.form_submit_button("Login")
            
            if submitted:
                if password == st.secrets.get("ADMIN_PASSWORD", "admin123"):
                    st.session_state.admin_authenticated = True
                    st.success("Login successful!")
                    st.rerun()
                else:
                    st.error("Invalid password")
        return False
    return True

def show_admin_dashboard():
    """Display admin dashboard"""
    st.markdown('<h1 class="admin-header">üîß AgentComponents Admin Dashboard</h1>', unsafe_allow_html=True)
    
    # Initialize API manager
    api_manager = FreeAPIManager()
    api_status = api_manager.get_api_status()
    
    # System Status
    st.markdown("## System Status")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown('<div class="status-good">‚úÖ App Status<br><strong>Running</strong></div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown('<div class="status-good">‚úÖ Database<br><strong>Connected</strong></div>', unsafe_allow_html=True)
    
    with col3:
        st.markdown('<div class="status-good">‚úÖ Security<br><strong>Active</strong></div>', unsafe_allow_html=True)
    
    with col4:
        api_live = any(status["available"] and status["under_limit"] for status in api_status.values())
        if api_live:
            st.markdown('<div class="status-good">‚úÖ APIs<br><strong>Live</strong></div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="status-warning">‚ö†Ô∏è APIs<br><strong>Limited</strong></div>', unsafe_allow_html=True)
    
    # Feature Status
    st.markdown("## Feature Status")
    
    # API Status Details
    gemini_status = api_status["gemini"]
    hf_status = api_status["huggingface"]
    
    if gemini_status["available"]:
        if gemini_status["under_limit"]:
            st.markdown(f'<div class="status-good">‚úÖ Gemini AI Research: Available ({gemini_status["usage_today"]}/80 today)</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="status-warning">‚ö†Ô∏è Gemini AI Research: Daily limit reached</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="status-error">‚ùå Gemini AI Research: Not configured</div>', unsafe_allow_html=True)
    
    if hf_status["available"]:
        if hf_status["under_limit"]:
            st.markdown(f'<div class="status-good">‚úÖ Content Generation: Available ({hf_status["usage_today"]}/25 today)</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="status-warning">‚ö†Ô∏è Content Generation: Daily limit reached</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="status-error">‚ùå Content Generation: Not configured</div>', unsafe_allow_html=True)
    
    # Environment Configuration
    st.markdown("## Environment Configuration")
    
    env_vars = [
        ("SUPABASE_URL", "‚úÖ Configured"),
        ("SUPABASE_ANON_KEY", "‚úÖ Configured"),
        ("GEMINI_API_KEY", "‚úÖ Configured" if st.secrets.get("GEMINI_API_KEY") else "‚ùå Missing"),
        ("HUGGINGFACE_TOKEN", "‚úÖ Configured" if st.secrets.get("HUGGINGFACE_TOKEN") else "‚ùå Missing"),
        ("ADMIN_PASSWORD", "‚úÖ Configured")
    ]
    
    for var, status in env_vars:
        color = "status-good" if "‚úÖ" in status else "status-error"
        st.markdown(f'<div class="{color}">{status} {var}</div>', unsafe_allow_html=True)
    
    # Usage Statistics
    st.markdown("## Usage Statistics")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        total_requests = gemini_status["usage_today"] + hf_status["usage_today"]
        st.metric("Total Requests Today", total_requests)
    
    with col2:
        st.metric("Unique Users", "0")
    
    with col3:
        success_rate = "100%" if total_requests == 0 else "95%"
        st.metric("Success Rate", success_rate)
    
    # Admin Actions
    st.markdown("## Admin Actions")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Refresh Status"):
            st.rerun()
    
    with col2:
        if st.button("üß™ Test APIs"):
            with st.spinner("Testing APIs..."):
                test_results = test_api_connections(api_manager)
                for api, result in test_results.items():
                    if result["success"]:
                        st.success(f"‚úÖ {api}: {result['message']}")
                    else:
                        st.error(f"‚ùå {api}: {result['message']}")

def test_api_connections(api_manager):
    """Test API connections"""
    results = {}
    
    # Test Gemini
    try:
        if api_manager.gemini_model and api_manager.check_daily_limit('gemini'):
            test_research = api_manager.research_with_gemini("test topic")
            if test_research:
                results["Gemini"] = {"success": True, "message": "Research API working"}
            else:
                results["Gemini"] = {"success": False, "message": "Research failed - no response"}
        else:
            results["Gemini"] = {"success": False, "message": "Not available or limit reached"}
    except Exception as e:
        results["Gemini"] = {"success": False, "message": f"Error: {str(e)[:100]}..."}
    
    # Test Hugging Face with detailed error reporting
    try:
        if api_manager.hf_client and api_manager.check_daily_limit('huggingface'):
            # Try direct API call first for better error reporting
            try:
                import requests
                
                api_url = "https://api-inference.huggingface.co/models/gpt2"
                headers = {"Authorization": f"Bearer {st.secrets.get('HUGGINGFACE_TOKEN')}"}
                
                payload = {
                    "inputs": "Test LinkedIn post about AI:",
                    "parameters": {
                        "max_new_tokens": 30,
                        "temperature": 0.7,
                        "return_full_text": False
                    }
                }
                
                response = requests.post(api_url, headers=headers, json=payload, timeout=15)
                
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        results["Hugging Face"] = {"success": True, "message": "Direct API working"}
                    else:
                        results["Hugging Face"] = {"success": False, "message": f"API returned: {result}"}
                elif response.status_code == 503:
                    results["Hugging Face"] = {"success": False, "message": "Model loading (503) - try again in a moment"}
                else:
                    results["Hugging Face"] = {"success": False, "message": f"HTTP {response.status_code}: {response.text[:100]}"}
                    
            except requests.exceptions.Timeout:
                results["Hugging Face"] = {"success": False, "message": "Request timeout - model may be loading"}
            except Exception as e:
                # Try InferenceClient as backup
                try:
                    test_content = api_manager.generate_content_with_hf("AI")
                    if test_content:
                        results["Hugging Face"] = {"success": True, "message": "InferenceClient working"}
                    else:
                        results["Hugging Face"] = {"success": False, "message": "InferenceClient failed - no content"}
                except Exception as e2:
                    results["Hugging Face"] = {"success": False, "message": f"Both methods failed: {str(e)[:50]}"}
        else:
            if not api_manager.hf_client:
                results["Hugging Face"] = {"success": False, "message": "Client not initialized - check HUGGINGFACE_TOKEN"}
            else:
                results["Hugging Face"] = {"success": False, "message": "Daily limit reached"}
    except Exception as e:
        results["Hugging Face"] = {"success": False, "message": f"Setup error: {str(e)[:50]}"}
    
    return results

def main_app():
    """Main application interface"""
    st.markdown('<h1 class="main-header">ü§ñ LinkedIn Content Intelligence Agent</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div style="text-align: center; padding: 1rem; background: rgba(255,255,255,0.1); border-radius: 10px; margin-bottom: 2rem;">
        <p style="font-size: 1.1rem; color: #666;">
            Research trending topics and generate data-driven LinkedIn content with AI-powered insights
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize API manager
    api_manager = FreeAPIManager()
    api_status = api_manager.get_api_status()
    
    # Show API status
    api_live = any(status["available"] and status["under_limit"] for status in api_status.values())
    if api_live:
        st.success("üü¢ Live AI research and content generation available")
    else:
        st.warning("üü° Running on cached examples (daily API limits reached)")
    
    # Main interface
    with st.container():
        st.markdown("### üîç Content Research & Generation")
        
        topic = st.text_input(
            "Enter a topic for LinkedIn content research:",
            placeholder="e.g., AI in healthcare, remote work trends, sustainable business practices"
        )
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            generate_btn = st.button("üöÄ Research & Generate Content", type="primary")
        
        with col2:
            use_cached = st.checkbox("Use cached examples", value=not api_live)
    
    if generate_btn and topic:
        with st.spinner("üîç Researching topic and generating content..."):
            
            if use_cached or not api_live:
                # Use cached examples
                cached_examples = load_cached_examples()
                
                # Find best match or use first example
                example_key = topic if topic in cached_examples else list(cached_examples.keys())[0]
                example = cached_examples[example_key]
                
                st.markdown("### üìä Research Summary")
                st.info("‚ÑπÔ∏è Showing cached research data")
                
                research_data = example["research"]
                for key, value in research_data.items():
                    if isinstance(value, list):
                        st.markdown(f"**{key.title()}:**")
                        for item in value:
                            st.markdown(f"‚Ä¢ {item}")
                    else:
                        st.markdown(f"**{key.title()}:** {value}")
                
                st.markdown("### ‚úçÔ∏è Generated LinkedIn Content")
                st.info("‚ÑπÔ∏è Showing example content")
                
                content = example["content"]
                st.markdown("**Final LinkedIn Post:**")
                st.code(content, language="")
                
                # Copy button
                st.markdown("**Copy to clipboard:**")
                st.text_area("Generated Content", value=content, height=200, label_visibility="collapsed")
                
            else:
                # Use live APIs
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("### üìä Research Summary")
                    research_data = api_manager.research_with_gemini(topic)
                    
                    if research_data:
                        st.success("‚úÖ Research completed with Gemini AI")
                        # Display the research summary with proper text wrapping
                        if "research_summary" in research_data:
                            st.markdown(f'<div class="research-container">{research_data["research_summary"]}</div>', unsafe_allow_html=True)
                    else:
                        st.warning("‚ö†Ô∏è Research failed - using cached example")
                        # Use cached research
                        cached_examples = load_cached_examples()
                        example_key = topic if topic in cached_examples else list(cached_examples.keys())[0]
                        example = cached_examples[example_key]
                        research_data = example["research"]
                        
                        for key, value in research_data.items():
                            if isinstance(value, list):
                                st.markdown(f"**{key.title()}:**")
                                for item in value:
                                    st.markdown(f"‚Ä¢ {item}")
                            else:
                                st.markdown(f"**{key.title()}:** {value}")
                
                with col2:
                    st.markdown("### ‚úçÔ∏è Generated Content")
                    content = api_manager.generate_content_with_hf(topic, research_data)
                    
                    if content:
                        st.success("‚úÖ Content generated with Hugging Face")
                        st.markdown("**LinkedIn Post:**")
                        st.code(content, language="")
                        
                        # Copy button
                        st.markdown("**Copy to clipboard:**")
                        st.text_area("", value=content, height=150)
                    else:
                        st.warning("‚ö†Ô∏è Content generation failed - showing cached example")
                        # Fallback to cached
                        cached_examples = load_cached_examples()
                        example_key = topic if topic in cached_examples else list(cached_examples.keys())[0]
                        example = cached_examples[example_key]
                        fallback_content = example["content"]
                        st.code(fallback_content, language="")
                        
                        # Copy button for fallback
                        st.markdown("**Copy to clipboard:**")
                        st.text_area("Generated Content", value=fallback_content, height=150, label_visibility="collapsed")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 1rem;">
        <p>ü§ñ Powered by AgentComponents | Using Free AI APIs (Gemini + Hugging Face)</p>
        <p>Daily limits: Gemini (80 requests) | Hugging Face (25 requests)</p>
    </div>
    """, unsafe_allow_html=True)

def main():
    """Main application entry point"""
    
    # Check for admin access
    if check_admin_access():
        if admin_login():
            show_admin_dashboard()
    else:
        main_app()

if __name__ == "__main__":
    main()
